# -*- coding: utf-8 -*-
"""Celonis_Customer_Support_AI_Colab.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1r3zYD3Q3xVRKj6Ks8iWkOWA-B-69WeG7

####Install Dependencies
"""

!pip install -q sentence-transformers transformers accelerate datasets
!pip install -q chromadb fastapi uvicorn nest_asyncio pyngrok

"""####Import Libraries"""

from sentence_transformers import SentenceTransformer
import chromadb
from chromadb.utils import embedding_functions
from datasets import load_dataset
from transformers import pipeline
import uuid
import os
import csv
from datetime import datetime
from fastapi import FastAPI
from pydantic import BaseModel
from fastapi.responses import JSONResponse
from fastapi.middleware.cors import CORSMiddleware
from chromadb.config import Settings

"""Secure Your Hugging Face Token"""

from huggingface_hub import login
from getpass import getpass

# Securely enter your Hugging Face Token
hf_token = getpass("Enter your Hugging Face token: ")

# Login using the token
login(token=hf_token)

"""####Secure your ngrok Token"""

# For security, we prompt ngrok token using getpass instead of hardcoding
#This is safer and avoids leaking credentials in shared notebooks
from getpass import getpass

#Secure token input (won't show in notebook)
authtoken = getpass("Enter your ngrok token: ")

#Add token to ngrok config
!ngrok config add-authtoken {authtoken}

"""Load Dataset and Setup Vector Database"""

# Load a subset of customer support tweets from Hugging Face dataset
dataset = load_dataset("MohammadOthman/mo-customer-support-tweets-945k", split="train[:1000]")

# Extract query-response pairs from the dataset
query_response_pairs = []
for item in dataset:
    if item.get("input") and item.get("output"):
        query = item["input"].strip()
        response = item["output"].strip()
        query_response_pairs.append({"query": query, "response": response})

# Load sentence embedding model
embedding_model = SentenceTransformer("all-MiniLM-L6-v2")

# Configure ChromaDB persistent storage
client_settings = Settings(persist_directory="./chroma_db")
chroma_client = chromadb.Client(settings=client_settings)

# Define the embedding function for vector search
embedding_fn = embedding_functions.SentenceTransformerEmbeddingFunction(model_name="all-MiniLM-L6-v2")

# Create or load the ChromaDB collection
try:
    collection = chroma_client.get_collection("support_bot")
except:
    collection = chroma_client.create_collection(name="support_bot", embedding_function=embedding_fn)

# Populate the collection with embeddings if empty
if collection.count() == 0:
    for pair in query_response_pairs:
        doc_id = str(uuid.uuid4())
        collection.add(
            documents=[f"{pair['query']} {pair['response']}"],
            metadatas=[{"response": pair["response"]}],
            ids=[doc_id]
        )

"""####Load Mistral-7B"""

generator = pipeline(
    "text-generation",
    model="mistralai/Mistral-7B-Instruct-v0.1",

    device_map="auto",
    model_kwargs={"torch_dtype": "auto"}
)

"""FastAPI Service for LLM Response Generation and Evaluation Logging"""

from fastapi import FastAPI
from pydantic import BaseModel
from fastapi.responses import JSONResponse
from fastapi.middleware.cors import CORSMiddleware
from datetime import datetime
from typing import Optional
import csv
import re

app = FastAPI()

# Allow API access from browser / Swagger / external frontend
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_methods=["*"],
    allow_headers=["*"]
)

# Request schema for generating LLM-based response
class QueryInput(BaseModel):
    query: str

# Request schema for submitting manual scores
class ScoreInput(BaseModel):
    query: str
    answer: str
    top_document: str
    relevance: int
    correctness: int
    conciseness: int
    fluency: int

# Endpoint: Generate model response and auto-score with LLM-as-judge
@app.post("/generate_response")
async def generate_response(payload: QueryInput):
    user_query = payload.query

    # Vector-based document retrieval (top 3)
    results = collection.query(query_texts=[user_query], n_results=3)
    retrieved_docs = results["documents"][0]
    top_doc = retrieved_docs[0]
    context = "\n\n".join(retrieved_docs)

    # Main prompt for response generation
    prompt = f"""<s>[INST] <<SYS>>
You are a customer support agent. Answer in 2-3 short, polite sentences.
Avoid repetition and ask for clarification if needed.
<</SYS>>

Context:
{context}

User Query:
{user_query}

Give a direct, non-repeating answer. [/INST]"""

    # Generate model output
    try:
        generated = generator(
            prompt,
            max_new_tokens=100,
            temperature=0.2,
            do_sample=True,
            repetition_penalty=2.0,
            no_repeat_ngram_size=3,
            early_stopping=True
        )[0]["generated_text"]
    except Exception as e:
        return JSONResponse(status_code=500, content={"error": str(e)})

    # Extract the generated answer from the output
    answer = generated.split("[/INST]")[-1].strip()
    if answer.count(".") > 2:
        answer = answer.split(".")[0] + "."

    # Create evaluation prompt (must match regex expectation)
    judge_prompt = f"""<s>[INST] <<SYS>>
You are an expert grader. Strictly rate the chatbot response on 4 parameters:

Return scores in this exact format (only numbers):
Relevance: [1-5]
Correctness: [1-5]
Conciseness: [1-5]
Fluency: [1-5]

Do NOT explain anything. Just output the scores exactly like above.
<</SYS>>

Query: {user_query}

Context:
{context}

Response:
{answer}
[/INST]"""

    # LLM evaluates its own answer
    try:
        judge_response = generator(
            judge_prompt,
            max_new_tokens=100,
            temperature=0
        )[0]["generated_text"]
    except Exception as e:
        judge_response = ""
        print("Judge model failed:", e)

    # Extract scores using strict pattern matching
    match = re.search(
        r"Relevance:\s*(\d+)\s*Correctness:\s*(\d+)\s*Conciseness:\s*(\d+)\s*Fluency:\s*(\d+)",
        judge_response
    )
    if match:
        relevance, correctness, conciseness, fluency = match.groups()
    else:
        relevance = correctness = conciseness = fluency = "-"

    # Save evaluation log to CSV
    with open("response_eval_log.csv", mode="a", newline="") as file:
        writer = csv.writer(file)
        writer.writerow([
            datetime.now(), user_query, answer, top_doc,
            relevance, correctness, conciseness, fluency, "auto"
        ])

    # Return structured output
    return JSONResponse(content={
        "query": user_query,
        "answer": answer,
        "top_document": top_doc,
        "context": context,
        "evaluation_source": "auto",
        "evaluation": {
            "relevance": relevance,
            "correctness": correctness,
            "conciseness": conciseness,
            "fluency": fluency
        }
    })


# Endpoint: Submit manual scores (after human review)
@app.post("/submit_scores")
async def submit_manual_scores(payload: ScoreInput):
    try:
        with open("response_eval_log.csv", mode="a", newline="") as file:
            writer = csv.writer(file)
            writer.writerow([
                datetime.now(), payload.query, payload.answer, payload.top_document,
                payload.relevance, payload.correctness,
                payload.conciseness, payload.fluency, "manual"
            ])
        return {"status": "success", "message": "Manual scores saved successfully"}
    except Exception as e:
        return JSONResponse(status_code=500, content={"error": str(e)})

"""Expose FastAPI API on Public URL"""

from pyngrok import ngrok
import nest_asyncio
import uvicorn

public_url = ngrok.connect(8000)
print("ðŸš€ Public URL:", public_url)

nest_asyncio.apply()
uvicorn.run(app, host="0.0.0.0", port=8000)

"""Manually Check response_eval_log.csv"""

import pandas as pd

df = pd.read_csv("response_eval_log.csv", header=None)
df.columns = ["timestamp", "query", "answer", "top_doc", "relevance", "correctness", "conciseness", "fluency", "source"]
df.tail(5)